# -*- coding: utf-8 -*-
"""DS & ML Internship.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fa13Awu39VP5AVrhixyD1tC9kMaNCUxM

# **Week-2: Data Preprocessing**
"""

import pandas as pd

# Load the training and test datasets
train_df = pd.read_csv("/content/drive/MyDrive/train_dataset.csv")
test_df = pd.read_csv("/content/drive/MyDrive/test_dataset.csv")

# Display basic information about the training dataset
print("Training Dataset Info:")
print(train_df.info())
print("\nSample rows from Training Dataset:")
print(train_df.head())

# Display basic information about the test dataset
print("\nTest Dataset Info:")
print(test_df.info())
print("\nSample rows from Test Dataset:")
print(test_df.head())

# Summary statistics of numerical attributes in the training dataset
train_stats = train_df.describe()

# Display the summary statistics
print(train_stats)

# Check for missing values in the training dataset
train_missing_values = train_df.isnull().sum()
print("Missing values in training dataset:")
print(train_missing_values)

# Check for missing values in the test dataset
test_missing_values = test_df.isnull().sum()
print("\nMissing values in test dataset:")
print(test_missing_values)

# Checking for duplicates in the training dataset
duplicates_train = train_df.duplicated()
print("Number of duplicate rows in training dataset:", duplicates_train.sum())

# Removing duplicates from the training dataset
train_df = train_df[~duplicates_train]

# Checking for duplicates in the test dataset
duplicates_test = test_df.duplicated()
print("Number of duplicate rows in test dataset:", duplicates_test.sum())

# Removing duplicates from the test dataset
test_df = test_df[~duplicates_test]

import matplotlib.pyplot as plt

# Convert DateTime column to datetime type
train_df['DateTime'] = pd.to_datetime(train_df['DateTime'])

# Plotting the trend of vehicles over time
plt.figure(figsize=(12, 6))
plt.plot(train_df['DateTime'], train_df['Vehicles'])
plt.title('Trend of Vehicles Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Vehicles')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.barplot(x='Junction', y='Vehicles', data=train_df)
plt.title('Number of Vehicles by Junction')
plt.xlabel('Junction')
plt.ylabel('Number of Vehicles')
plt.show()

# Feature Engineering: Extracting date-time components
train_df['DateTime'] = pd.to_datetime(train_df['DateTime'])
test_df['DateTime'] = pd.to_datetime(test_df['DateTime'])

train_df['Year'] = train_df['DateTime'].dt.year
train_df['Month'] = train_df['DateTime'].dt.month
train_df['Day'] = train_df['DateTime'].dt.day
train_df['Hour'] = train_df['DateTime'].dt.hour
train_df['Weekday'] = train_df['DateTime'].dt.weekday

test_df['Year'] = test_df['DateTime'].dt.year
test_df['Month'] = test_df['DateTime'].dt.month
test_df['Day'] = test_df['DateTime'].dt.day
test_df['Hour'] = test_df['DateTime'].dt.hour
test_df['Weekday'] = test_df['DateTime'].dt.weekday

# Convert 'Junction' to categorical
train_df['Junction'] = train_df['Junction'].astype('category')
test_df['Junction'] = test_df['Junction'].astype('category')

# Drop the original 'DateTime' column
train_df.drop(columns=['DateTime'], inplace=True)
test_df.drop(columns=['DateTime'], inplace=True)

# Display the updated dataset
print(train_df.head())
print(test_df.head())

'''from sklearn.preprocessing import MinMaxScaler

# Initialize the scaler
scaler = MinMaxScaler()

# Columns to be scaled
columns_to_scale = ['Year', 'Month', 'Day', 'Hour', 'Weekday']

# Scale the features in training dataset
train_df[columns_to_scale] = scaler.fit_transform(train_df[columns_to_scale])

# Scale the features in test dataset
test_df[columns_to_scale] = scaler.transform(test_df[columns_to_scale])

# Display the updated dataset
print(train_df.head())
print(test_df.head())'''

"""# **Week-3: Data Modelling**"""

from datetime import datetime
import holidays

# Initialize the India holidays
india_holidays = holidays.India()

# Function to check if a date is a holiday or an event
def is_holiday_or_event(date_str):
    date = datetime.strptime(date_str, "%Y-%m-%d")
    return int(date in india_holidays)

# Apply the function to the training dataset
train_df["IsHolidayOrEvent"] = train_df["Year"].astype(str) + "-" + train_df["Month"].astype(str) + "-" + train_df["Day"].astype(str)
train_df["IsHolidayOrEvent"] = train_df["IsHolidayOrEvent"].apply(is_holiday_or_event)

# Apply the function to the test dataset
test_df["IsHolidayOrEvent"] = test_df["Year"].astype(str) + "-" + test_df["Month"].astype(str) + "-" + test_df["Day"].astype(str)
test_df["IsHolidayOrEvent"] = test_df["IsHolidayOrEvent"].apply(is_holiday_or_event)

print(train_df.head())
print(test_df.head())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))
sns.barplot(x='IsHolidayOrEvent', y='Vehicles', data=train_df)
plt.xlabel('Is Holiday/Event')
plt.ylabel('Average Number of Vehicles')
plt.title('Average Number of Vehicles on Holidays vs Non-Holidays')
plt.xticks([0, 1], ['Non-Holiday', 'Holiday/Event'])
plt.show()

plt.figure(figsize=(12, 8))
sns.barplot(x='Junction', y='Vehicles', hue='IsHolidayOrEvent', data=train_df)
plt.xlabel('Junction')
plt.ylabel('Average Number of Vehicles')
plt.title('Average Number of Vehicles on Holidays vs Non-Holidays by Junction')
plt.legend(title='Is Holiday/Event', labels=['Non-Holiday', 'Holiday/Event'])
plt.show()

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Prepare the data for all models
features = ['Junction', 'Year', 'Month', 'Day', 'Hour', 'Weekday', 'IsHolidayOrEvent']
X = train_df[features]
y = train_df['Vehicles']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear Regression
linear_reg_model = LinearRegression()
linear_reg_model.fit(X_train, y_train)
y_pred_lr = linear_reg_model.predict(X_test)

# Decision Tree Regression
decision_tree_model = DecisionTreeRegressor(random_state=42)
decision_tree_model.fit(X_train, y_train)
y_pred_dt = decision_tree_model.predict(X_test)

# Random Forest Regression
random_forest_model = RandomForestRegressor(random_state=42)
random_forest_model.fit(X_train, y_train)
y_pred_rf = random_forest_model.predict(X_test)

# Support Vector Machine (SVM) Regression
svm_model = SVR()
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)

# Neural Network Regression
nn_model = MLPRegressor(random_state=42)
nn_model.fit(X_train, y_train)
y_pred_nn = nn_model.predict(X_test)

# Print the predictions for all models
print("Linear Regression Predictions:", y_pred_lr)
print("Decision Tree Predictions:", y_pred_dt)
print("Random Forest Predictions:", y_pred_rf)
print("SVM Predictions:", y_pred_svm)
print("Neural Network Predictions:", y_pred_nn)

# Calculate metrics for all models
def calculate_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mae, mse, r2

mae_lr, mse_lr, r2_lr = calculate_metrics(y_test, y_pred_lr)
mae_dt, mse_dt, r2_dt = calculate_metrics(y_test, y_pred_dt)
mae_rf, mse_rf, r2_rf = calculate_metrics(y_test, y_pred_rf)
mae_svm, mse_svm, r2_svm = calculate_metrics(y_test, y_pred_svm)
mae_nn, mse_nn, r2_nn = calculate_metrics(y_test, y_pred_nn)

# Print the metrics for all models
print("\nMetrics for Linear Regression:")
print("Mean Absolute Error:", mae_lr)
print("Mean Squared Error:", mse_lr)
print("R-squared:", r2_lr)
print("\nMetrics for Decision Tree:")
print("Mean Absolute Error:", mae_dt)
print("Mean Squared Error:", mse_dt)
print("R-squared:", r2_dt)
print("\nMetrics for Random Forest:")
print("Mean Absolute Error:", mae_rf)
print("Mean Squared Error:", mse_rf)
print("R-squared:", r2_rf)
print("\nMetrics for SVM:")
print("Mean Absolute Error:", mae_svm)
print("Mean Squared Error:", mse_svm)
print("R-squared:", r2_svm)
print("\nMetrics for Neural Network:")
print("Mean Absolute Error:", mae_nn)
print("Mean Squared Error:", mse_nn)
print("R-squared:", r2_nn)

''''from sklearn.model_selection import GridSearchCV

# Define hyperparameters to search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Create a GridSearchCV instance
grid_search = GridSearchCV(estimator=random_forest_model, param_grid=param_grid, cv=5)

# Fit the model to find the best hyperparameters
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_'''

import matplotlib.pyplot as plt

# Names of the models
model_names = ['Linear Regression', 'Decision Tree', 'Random Forest', 'SVM', 'Neural Network']

# Error metrics for each model
mae_values = [mae_lr, mae_dt, mae_rf, mae_svm, mae_nn]
mse_values = [mse_lr, mse_dt, mse_rf, mse_svm, mse_nn]
r2_values = [r2_lr, r2_dt, r2_rf, r2_svm, r2_nn]

# Plotting Mean Absolute Error (MAE)
plt.figure(figsize=(10, 5))
plt.bar(model_names, mae_values, color='blue')
plt.xlabel('Models')
plt.ylabel('Mean Absolute Error')
plt.title('Mean Absolute Error for Different Models')
plt.show()

# Plotting Mean Squared Error (MSE)
plt.figure(figsize=(10, 5))
plt.bar(model_names, mse_values, color='green')
plt.xlabel('Models')
plt.ylabel('Mean Squared Error')
plt.title('Mean Squared Error for Different Models')
plt.show()

# Plotting R-squared (R2)
plt.figure(figsize=(10, 5))
plt.bar(model_names, r2_values, color='purple')
plt.xlabel('Models')
plt.ylabel('R-squared')
plt.title('R-squared for Different Models')
plt.show()

from sklearn.model_selection import GridSearchCV


# Define hyperparameters to search through
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt']
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=random_forest_model, param_grid=param_grid,
                           scoring='neg_mean_squared_error', cv=3, verbose=2, n_jobs=-1)

# Perform the grid search
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

from sklearn.ensemble import RandomForestRegressor

# Create a Random Forest model with best hyperparameters
best_rf_model = RandomForestRegressor(max_depth=None, max_features=1.0, min_samples_leaf=1, min_samples_split=2, n_estimators=150, random_state=42)

# Fit the model to the training data
best_rf_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred_best_rf = best_rf_model.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Calculate metrics for the best Random Forest model
mae_best_rf = mean_absolute_error(y_test, y_pred_best_rf)
mse_best_rf = mean_squared_error(y_test, y_pred_best_rf)
r2_best_rf = r2_score(y_test, y_pred_best_rf)

# Print the metrics for the best Random Forest model
print("\nMetrics for Best Random Forest:")
print("Mean Absolute Error:", mae_best_rf)
print("Mean Squared Error:", mse_best_rf)
print("R-squared:", r2_best_rf)

import matplotlib.pyplot as plt

# Names of the models
model_names = ['Old Random Forest', 'New Random Forest']

# Error metrics for each model
mae_values = [mae_rf, mae_best_rf]
mse_values = [mse_rf, mse_best_rf]
r2_values = [r2_rf, r2_best_rf]

# Plotting Mean Absolute Error (MAE)
plt.figure(figsize=(6, 4))
plt.bar(model_names, mae_values, color='blue')
plt.xlabel('Models')
plt.ylabel('Mean Absolute Error')
plt.title('Mean Absolute Error Comparison: Old RF vs New RF')
plt.show()

# Plotting Mean Squared Error (MSE)
plt.figure(figsize=(6, 4))
plt.bar(model_names, mse_values, color='green')
plt.xlabel('Models')
plt.ylabel('Mean Squared Error')
plt.title('Mean Squared Error Comparison: Old RF vs New RF')
plt.show()

# Plotting R-squared (R2)
plt.figure(figsize=(6, 4))
plt.bar(model_names, r2_values, color='purple')
plt.xlabel('Models')
plt.ylabel('R-squared')
plt.title('R-squared Comparison: Old RF vs New RF')
plt.show()

X_unseen = test_df[features]

y_pred_unseen = best_rf_model.predict(X_unseen).round()


# Extract the ID column from the test dataset
predictions_data = {
    'ID': test_df['ID'],
    'Junction': test_df['Junction'],
    'Year': test_df['Year'],
    'Month': test_df['Month'],
    'Day': test_df['Day'],
    'Hour': test_df['Hour'],
    'Weekday': test_df['Weekday'],
    'IsHolidayOrEvent': test_df['IsHolidayOrEvent'],
    'Predicted_Vehicles': y_pred_unseen
}

# Create a DataFrame with the predictions
predictions_df = pd.DataFrame(predictions_data)

# Save the predictions to a CSV file
predictions_df.to_csv('predictions.csv', index=False)

predictions_df.head()

